diff --git a/auxi.py b/auxi.py
index 2a2277e..a0db2e8 100644
--- a/auxi.py
+++ b/auxi.py
@@ -27,6 +27,37 @@ import gkpose as gk
 direction_dict = {'bottom right corner':1, 'bottom left corner':2, 'centre of the goal':3,
 			'top left corner':4, 'top right corner':5, 'top centre of the goal':6,}
 
+def get_t_value(confidence=0.95, degrees_of_freedom=1, side='two-sided'):
+    '''
+        Retorna o t value dado uma confiança de x% e y graus de liberdade
+    '''
+    alfa = 1-confidence
+    if side=='two-sided':
+        return stats.t.ppf(q = 1-alfa/2,df=degrees_of_freedom)
+    elif side=='one-sided':
+        return stats.t.ppf(q = 1-alfa,df=degrees_of_freedom)
+    return -1
+
+def get_z_value(confidence=0.95, side='two-sided'):
+    alfa = 1-confidence
+    if side=='two-sided':
+        return stats.norm.ppf(q = 1-alfa/2)
+    elif side=='one-sided':
+        return stats.norm.ppf(q = 1-alfa)
+    return -1
+
+def get_f_value(confidence=0.95, dfn=1, dfd=1, side='two-sided'):
+    '''
+        Retorna o f value dado uma confiança de x%, dfn e dfd
+    '''
+    alfa = 1-confidence
+    if side=='two-sided':
+        return stats.f.ppf(q = 1-alfa/2, dfn=dfn, dfd=dfd)
+    elif side=='one-sided':
+        return stats.f.ppf(q = 1-alfa, dfn=dfn, dfd=dfd)
+    return -1
+
+
 def find_duplicates(l):
 	l=sorted(l)
 	l1, dups = set(), set()
@@ -161,14 +192,25 @@ def SpearmanCorrelation(y_true, y_pred):
 	
 	return rho, pval
 
-def regression_info(res, outcome, start='>'):
+def get_auto_ftest(res):
+	# Teste F
+	A = np.identity(len(res.params))[1:,:]
+	f_t = res.f_test(A)
+	f_value, max_confidence = f_t.fvalue, 1-f_t.pvalue
+	f_95 = get_f_value(confidence=0.95, dfn=f_t.df_num, dfd=f_t.df_denom, side='one-sided')
+	print("F-Table[95%]: {}; F-Value: {}, Max Conf Int: {}".format(f_95, f_value, max_confidence))
+	return f_95, f_value, max_confidence
+
+def regression_info(res, train, outcome, start='>'):
+
 
+	predicted = res.predict(train)
 
-	sse = np.sum((res.fittedvalues - outcome)**2)
+	sse = np.sum((predicted - outcome)**2)
 	# sse = res.ssr
 	print(start, "SSE:", sse)
 
-	ssr = np.sum((res.fittedvalues - outcome.mean())**2)
+	ssr = np.sum((predicted - outcome.mean())**2)
 	# ssr = res.ess
 	print(start, "SSR:", ssr)
 
@@ -178,14 +220,43 @@ def regression_info(res, outcome, start='>'):
 	rsquared = 1 - sse/sst
 	print(start, "R^2:", rsquared)
 
-	# y_pred_train = np.array(res.fittedvalues).reshape(-1, 1) #returns a numpy array
-	# min_max_scaler = MinMaxScaler()
-	# y_pred_train = min_max_scaler.fit_transform(y_pred_train)
-	# y_pred_train = y_pred_train.flatten()
+	try:
+		# Cálculo automático F-teste
+		f_95, f_value, max_confidence = get_auto_ftest(res)
+	except Exception as e:
+		dof_sse = res.df_resid
+		dof_ssr = res.df_model
 
-	# for t, p in zip(y_pred_train, outcome):
-	# 	print(t, p)
-	return sse, ssr, sst, rsquared
+		msr = ssr/dof_ssr
+		mse = sse/dof_sse
+
+		f_value = msr/mse
+		f_95 = get_f_value(0.95, dfn=dof_ssr, dfd=dof_sse, side='one-sided')
+		max_confidence = stats.f.cdf(f_value, dfn=dof_ssr, dfd=dof_sse)
+		print("F-Table[95%]: {}; F-Value: {}, Max Conf Int: {}".format(f_95, f_value, max_confidence))
+
+
+	p_values = res.pvalues.to_dict()
+	print("P-values:", p_values)		
+
+	# try:
+	# 	cov_matrix = res.cov_params()
+	# 	print(cov_matrix)
+	# except Exception as e:
+	# 	cov_matrix = train.corr()
+	# 	print(cov_matrix)
+
+	cov_matrix = train.corr()
+	print(cov_matrix)
+
+	['aic', 'bic', 'bse', 'conf_int', 'cov_kwds', 'cov_params', 'cov_type', 'df_model', 'df_resid', 
+	'f_test', 'fittedvalues', 'get_margeff', 'initialize', 'k_constant', 'llf', 'llnull', 'llr', 'llr_pvalue', 
+	'load', 'mle_retvals', 'mle_settings', 'model', 'nobs', 'normalized_cov_params', 'params', 'pred_table', 
+	'predict', 'prsquared', 'pvalues', 'remove_data', 'resid_dev', 'resid_generalized', 'resid_pearson', 
+	'resid_response', 'save', 'scale', 'set_null_options', 'summary', 'summary2', 't_test', 't_test_pairwise', 
+	'tvalues', 'use_t', 'wald_test', 'wald_test_terms']
+
+	return sse, ssr, sst, rsquared, f_95, f_value, max_confidence, p_values, cov_matrix
 
 # aux
 
@@ -534,14 +605,22 @@ def create_side_df(sets_2d, set_2d_df, save=False):
 		df.to_csv("data/events/side_1v1.csv", index=False)
 	return
 
-def PenaltyDummies(good_poses_feat_df, continuous_var):
+def PenaltyDummies(good_poses_feat_df, continuous_var, drop_top=True, dummies=True):
 
 	if 'Direction' in good_poses_feat_df.columns:
+		# Drop penalties on top zone, trying to fix regression
+		if drop_top:
+			good_poses_feat_df = good_poses_feat_df[good_poses_feat_df.Direction != 'top right corner']
+			good_poses_feat_df = good_poses_feat_df[good_poses_feat_df.Direction != 'top left corner']
+			good_poses_feat_df = good_poses_feat_df[good_poses_feat_df.Direction != 'top centre of the goal']
+		
 		good_poses_feat_df['Direction'] = good_poses_feat_df['Direction'].replace(direction_dict)
-		good_poses_feat_df = pd.get_dummies(good_poses_feat_df, columns=['Direction'])
+		if dummies:
+			good_poses_feat_df = pd.get_dummies(good_poses_feat_df, columns=['Direction'])
 	if 'Dominant_side' in good_poses_feat_df.columns:
 		good_poses_feat_df['Dominant_side'] = good_poses_feat_df['Dominant_side'].replace({'right': 0, 'left': 1})
-		good_poses_feat_df = pd.get_dummies(good_poses_feat_df, columns=['Dominant_side'])
+		if dummies:
+			good_poses_feat_df = pd.get_dummies(good_poses_feat_df, columns=['Dominant_side'])
 
 	return good_poses_feat_df.drop(columns=['indx'])
 
@@ -665,7 +744,7 @@ def cluster_correspondence(kmeans_preds, set_3d_cvi_clean_df, cluster_name, star
 			print("Equal??", perc_gt==perc_novo)
 			return dict((v,k) for k,v in s_perc_novo.items())
 	else:
-		print("HERE")
+		# print("HERE")
 		side_df = pd.read_csv("data/events/side_1v1.csv")
 		gt = gt.merge(side_df, on='img_id')
 		df = df.merge(side_df, on='img_id')
diff --git a/gkpose.py b/gkpose.py
index 6afea5e..367a4fb 100644
--- a/gkpose.py
+++ b/gkpose.py
@@ -255,22 +255,22 @@ def getxSInput(df, scaler, angle, dist, up=0, cluster=0):
 
 def getXSMap(train_df, model, scaler, num_clusters, up=0, ass='Pass'):
     #Sets: Probability Map
+    print("\t-> Probability map for UP={}".format(up))
     x_range = np.linspace(90, 120.01, 50)
     y_range = np.linspace(0, 80, 50)
     xs_map = np.zeros((num_clusters, len(x_range), len(y_range)))
+
     for cluster in range(num_clusters):
         for x in range(len(x_range)):
             for y in range(len(y_range)):
                 d = distance_to_goal(shooter_x=x_range[x], shooter_y=y_range[y])
                 a = goal_angle(shooter_x=x_range[x], shooter_y=y_range[y])
-                xs = []
-                for n in range(num_clusters):
-                    inp = getxSInput(train_df,scaler,angle=a,dist=d,up=up,cluster=n)
-                    xs.append(model.predict_proba(inp)[0][1])
-                mean_xs = np.mean(xs)
                 inp = getxSInput(train_df,scaler,angle=a,dist=d,up=up,cluster=cluster)
-                xs_map[cluster][x, y] = model.predict_proba(inp)[0][1] - mean_xs
+                xs_result = model.predict_proba(inp)[0][1]
+                xs_map[cluster][x, y] = xs_result # - mean_xs
         print("done cluster", cluster)
+
+    xs_map = xs_map - np.mean(xs_map, axis=0) #reduce complexity
     return xs_map
 
 def getGKEM(amateur_1v1s):
diff --git a/main.py b/main.py
index 317ae14..16c0d15 100644
--- a/main.py
+++ b/main.py
@@ -3,6 +3,7 @@ import warnings
 warnings.simplefilter(action='ignore', category=FutureWarning)
 
 # Imports
+import os
 import wandb
 import argparse
 import numpy as np
@@ -15,6 +16,8 @@ from sklearn.model_selection import GridSearchCV
 from sklearn.preprocessing import StandardScaler
 from sklearn.metrics import pairwise_distances_argmin_min, confusion_matrix, f1_score, recall_score, precision_score
 
+from imblearn.under_sampling import NearMiss
+
 import statsmodels.api as sm
 
 import auxi
@@ -182,6 +185,7 @@ def LearningSaveTechnique(sets_3d_cvi_clean, set_3d_cvi_clean_df, args, start='\
 	if args.show:
 		plots.plot_cluster(sets_3d_cvi_clean, set_3d_cvi_clean_df, closest, cluster_name, path='images/1v1_images/', show=args.show)
 
+	print("AQUIIII")
 	cluster_mean_df = auxi.mean_pose_per_cluster(kmeans_preds, set_3d_cvi_clean_df, cluster_dict, c_center)
 
 	if number_cluster == 8:
@@ -210,6 +214,12 @@ def LearningSaveTechnique(sets_3d_cvi_clean, set_3d_cvi_clean_df, args, start='\
 
 		wandb.log({"KMeans 1v1 Table": auxi.make_kmeans_df(kmeans_preds, set_3d_cvi_clean_df, cluster_dict)})
 		wandb_LearningSaveTechnique(TSNE_df, c_size_d, cluster_mean_df)
+
+	baseline = {'Aggressive Set':0, 'Passive Set':1, 'Spread':2 , 'Smother':3}
+	kmeans_preds = kmeans_preds + 4
+	replace = {k+4:baseline[v] for k,v in cluster_dict.items()}
+	for k, v in replace.items():
+		kmeans_preds = np.where(kmeans_preds==k, v, kmeans_preds)
 	return kmeans_preds
 
 def wandb_LearningSaveTechnique(TSNE_df, c_size_d, cluster_mean_df):
@@ -282,22 +292,30 @@ def ExpectedSavesModel_1v1(set_3d_cvi_clean_df, kmeans_preds, args, start='\t>')
 	
 	y_pred = svm.predict(X_test)
 	auxi.print_classification_metrics(y_test, y_pred, start)
-	
 
 	#Calculate xS map when striker is not under pressure
-	# xs_map = gk.getXSMap(train_df, svm, scaler, num_clusters=number_cluster, up=0) #ADD
+	xs_map = gk.getXSMap(train_df, svm, scaler, num_clusters=number_cluster, up=0) #ADD
+	
+	# best_tech_xs_map = np.argmax(xs_map, axis=0)
 
 	#Calculate xS map for when striker is under pressure
-	# xs_map_up = gk.getXSMap(train_df, svm, scaler, num_clusters=number_cluster, up=1) #ADD
+	xs_map_up = gk.getXSMap(train_df, svm, scaler, num_clusters=number_cluster, up=1) #ADD
 
+	cluster_name = ['Aggressive Set', 'Passive Set', 'Spread', 'Smother']
 	if args.show:
-		cluster_name = ['Aggressive Set', 'Passive Set', 'Spread', 'Smother']
 		plots.plotDoubleXSMap(xs_map, xs_map_up, cluster_name, show=args.show)
-
-		#Optimal technique map
+		# Optimal technique map
 		plots.plotBestTechniqueUp(xs_map, xs_map_up, cluster_name, show=args.show)
 
 	if not args.debug:
+		wandb.log({"XS Map": plots.plotBestTechniqueUp(xs_map, xs_map_up, cluster_name, show=args.show)})
+		# Save to pc
+		run_type = "v1{}-nd{}-g{}-si{}".format(args.view_invariant1, args.number_dimensions, args.grid_search, args.split_sides)
+		file_number = len([name for name in os.listdir('./results') if os.path.isfile(name)])/2
+		run_type = "{}-{}.npy".format(run_type, file_number)
+		np.save("results/xs_map"+run_type, xs_map)
+		np.save("results/xs_map_up"+run_type, xs_map_up)
+
 		wandb_ExpectedSavesModel_1v1(svm, X_test, y_test)
 
 	return train_gk_name, test_gk_name, train_df, test_df, svm
@@ -317,6 +335,7 @@ def wandb_ExpectedSavesModel_1v1(svm, X_test, y_test):
 	
 	d = {"conf_mat 1v1": cm, "Accuracy 1v1": test_set_acc, 'F1 1v1': f1, "Recall 1v1": recall, "Precision 1v1": precision}
 	wandb.log(d)
+	
 	return
 
 # Pro Goalkeeper Scouting
@@ -446,54 +465,68 @@ def PenaltyAnalysis(args, start='>'):
 	# Create dataframe of good poses features
 	good_poses_feat_df, continuous_var, formula = auxi.createPenaltyGoodPosesFeatures(poses_features, good_poses_3d_df, args)
 
+	# for var in continuous_var: #good_poses_feat_df.columns.tolist()
+	# 	plots.plot_scatter(good_poses_feat_df, x=var)
+	
+
 	# Create dummies for categorical features
-	good_poses_feat_df = auxi.PenaltyDummies(good_poses_feat_df, continuous_var)
+	good_poses_feat_df = auxi.PenaltyDummies(good_poses_feat_df, continuous_var, drop_top=True, dummies=True)
+	print(start, "New Percentage of saved penalties:", np.mean(good_poses_feat_df['outcome'] == 1) * 100)
 	
 	# Train/Test Split (70/30)
 	df = good_poses_feat_df.copy()
 	train_df = df.sample(frac=0.7,random_state=200) #random state is a seed value
 	test_df = df.drop(train_df.index)
-	# print(train_df.Dominant_side.value_counts())
-	# print(test_df.outcome.value_counts())
-	# print("Test Index", test_df.index)
+
+	X_train, y_train = train_df[train_df.columns[1:]], train_df['outcome']
+
+	# Resampling - because of class imbalance
+	undersample = NearMiss(0.4, version=3, n_neighbors=2)
+	X_train, y_train = undersample.fit_resample(X_train, y_train)
 
 	#Standardise continuous variables
 	scaler = StandardScaler()
-	scaler.fit(train_df[continuous_var])
-	train_df[continuous_var] = scaler.transform(train_df[continuous_var])
+	scaler.fit(X_train[continuous_var])
+	X_train[continuous_var] = scaler.transform(X_train[continuous_var])
 	test_df[continuous_var] = scaler.transform(test_df[continuous_var])
 	
+	X_test, y_test = test_df[test_df.columns[1:]].copy(), test_df['outcome']
+
 	# Add intercept term
-	train_df['coef'], test_df['coef'] = 1, 1
+	X_train = sm.add_constant(X_train)
+	X_test = sm.add_constant(X_test)
 
 	# Train logistic regression
-	log_reg = sm.Logit(train_df['outcome'], train_df[train_df.columns[1:]]).fit()
+	log_reg = sm.Logit(y_train, X_train).fit(method='bfgs', maxiter=100)	
 	# Logistic regression summary
 	print(log_reg.summary())
-	sse, ssr, sst, rsquared = auxi.regression_info(log_reg, train_df['outcome'], start)
+
+	sse, ssr, sst, rsquared, f_95, f_value, max_confidence, p_values, cov_matrix = auxi.regression_info(log_reg, X_train, y_train, start)
 	# Predictions
-	y_pred = log_reg.predict(test_df[test_df.columns[1:]])
-	
+	y_pred = log_reg.predict(X_test)
 	# Prediction stats
 	auxi.printPredictionStats(y_pred, test_df, start)
 
-	# # Train Linear Regression
-	# res = sm.OLS(train_df['outcome'], train_df[train_df.columns[1:]]).fit()
-	# print(res.summary())
-	# sse, ssr, sst, rsquared = auxi.regression_info(res, train_df['outcome'])
-	# # Predictions
-	# y_pred = res.predict(test_df[test_df.columns[1:]])
-	# # Prediction stats
-	# auxi.printPredictionStats(y_pred, test_df)
 
 	# summary = log_reg.summary2()
 	if not args.debug:
 		TSNE_df = auxi.createTSNEdf(pens_tsne, kmeans_pens_preds, {0:'Cluster 0', 1:'Cluster 1'})
+		
+		# Log model info
 		results_d = {"SSE":sse, "SSR":ssr, "SST":sst, "R-squared":rsquared}
-		wandbPenaltyAnalysis(y_pred, test_df, log_reg, results_d, TSNE_df)
+		wandb.log(results_d)
+		results_f = {"F-Table[95%]": f_95, "F-Value": f_value, "Max Conf Int": max_confidence}
+		wandb.log(results_f)
+		wandb.log(p_values)
+		table = wandb.Table(data=cov_matrix.values.tolist(), columns=cov_matrix.columns.tolist())
+		wandb.log({"Cov Matrix": table})
+
+
+		wandbPenaltyAnalysis(y_pred, test_df, log_reg, TSNE_df)
+
 	return
 
-def wandbPenaltyAnalysis(y_pred, test_df, model, results_d, TSNE_df):
+def wandbPenaltyAnalysis(y_pred, test_df, model, TSNE_df):
 	
 	def TSNE_plot():
 		data = TSNE_df.values.tolist()
@@ -501,8 +534,7 @@ def wandbPenaltyAnalysis(y_pred, test_df, model, results_d, TSNE_df):
 		wandb.log({"TSNE Penalty" : wandb.plot.scatter(table, 't-SNE_1', 't-SNE_2')})
 
 	TSNE_plot()
-	# Log model info
-	wandb.log(results_d)
+	
 	
 	# Log summary as table
 	summary = model.summary2()
@@ -573,4 +605,15 @@ if __name__ == '__main__':
 
 	# Penalty Analysis
 	print("\n\n\t========== PENALTY ANALYSIS ==========\t")
-	PenaltyAnalysis(args)
\ No newline at end of file
+	PenaltyAnalysis(args)
+
+
+
+	# # Train Linear Regression
+	# res = sm.OLS(train_df['outcome'], train_df[train_df.columns[1:]]).fit()
+	# print(res.summary())
+	# sse, ssr, sst, rsquared = auxi.regression_info(res, train_df['outcome'])
+	# # Predictions
+	# y_pred = res.predict(test_df[test_df.columns[1:]])
+	# # Prediction stats
+	# auxi.printPredictionStats(y_pred, test_df)
\ No newline at end of file
diff --git a/plots.py b/plots.py
index af3316f..61c6f45 100644
--- a/plots.py
+++ b/plots.py
@@ -80,6 +80,10 @@ def remove_labels(ax, s=2):
         ax.set_zlabel('')
 
 # Plots
+def plot_scatter(df, x):
+    df.plot.scatter(x, y='outcome')
+    plt.show()
+    return
 
 def plot_rectangle(points, bbox, show=False):
     '''
@@ -253,7 +257,7 @@ def plotBestTechniqueUp(xs_map, xs_map_up, cluster_name, show=False):
         Best technique to use
     '''
 
-    pitch = VerticalPitch(half=True, goal_type='box', pad_bottom=-37, pad_left=-15, pad_right=-15, line_color='black', orientation='horizontal')
+    pitch = VerticalPitch(half=True, goal_type='box', pad_bottom=-37, pad_left=-15, pad_right=-15, line_color='black')#, orientation='horizontal')
     fig, ax = pitch.draw(figsize=(10,5), nrows=1, ncols=2)
     
     cmap = plt.cm.tab20
@@ -274,6 +278,7 @@ def plotBestTechniqueUp(xs_map, xs_map_up, cluster_name, show=False):
     plt.tight_layout()
     if show:
         plt.show()
+    return plt
 
 def plotTSNE(pose_tsne, kmeans_preds, cluster_name, number=4, show=False):
     '''
diff --git a/run_1v1.sh b/run_1v1.sh
index f6e4ed9..37b0773 100644
--- a/run_1v1.sh
+++ b/run_1v1.sh
@@ -2,8 +2,7 @@
 
 view_invariant1=(1 0)
 number_dim=(2 3)
-# split_side=(0 1)
-split_side=(0)
+split_side=(0 1)
 grid_search=(0 1)
 
 
@@ -13,7 +12,7 @@ for v1 in ${view_invariant1[@]}; do
        for gs in ${grid_search[*]}; do
            for si in ${split_side[*]}; do
                echo "python main.py -v1 $v1 -nd $nd -g $gs -si $si"
-               # python main.py -v1 $v1 -nd $nd -g $gs -si $si
+               python main.py -d -v1 $v1 -nd $nd -g $gs -si $si
            done
        done
    done
diff --git a/run_penalty.sh b/run_penalty.sh
index b1f6ce1..1108171 100644
--- a/run_penalty.sh
+++ b/run_penalty.sh
@@ -11,7 +11,7 @@ for v2 in ${view_invariant2[@]}; do
        for h in ${hand[*]}; do
            for hei in ${height[*]}; do
                echo "python main.py -v2 $v2 -p $p -ha $h -he $hei"
-               # python main.py -v2 $v2 -p $p -ha $h -he $hei
+               python main.py -d -v2 $v2 -p $p -ha $h -he $hei
            done
        done
    done
